{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyric Generator!\n",
    "\n",
    "Further to my studies of NLP, I decided to see whether I could harness AI to write new lyrics based on my previous listening habits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I pulled my data from the Spotify API. Initially I was going to use only my top 100 songs from 2019 (according to Spotify) but I decided to use the same from 2018 as well in order to increased the size of the corpus used to train my model. I have left the code here for reference, but you can find the data that I actually used here. \n",
    "\n",
    "In order to run my notebook, please download the dataset and put it in the data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import spotipy.util as util\n",
    "import spotipy\n",
    "import os.path as path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from better_profanity import profanity\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import GRU, Dense, Input, Embedding, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "username = 'ninahew'\n",
    "spotify_client_id ='a758a8092e3b45949388c4123f79948f'\n",
    "spotify_client_secret = 'df4c710d84b64ab7810f0b9b055a07f2'\n",
    "redirect_uri = 'http://localhost:7777/callback'\n",
    "scope = 'user-read-recently-played'\n",
    "genius_key = 'sQkNdeBg0aT9rz89yElRfHbxE_UIW3kFs9IkXPIH6Ab4cH8DvfikT09FU56MVjZP'\n",
    "spotify_playlist_id = 'spotify:playlist:37i9dQZF1EtmAVggpyt82c'\n",
    "spotify_user_id = 'spotify:user:ninahew'\n",
    "\n",
    "\n",
    "# spotify = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(client_id=client_id, \n",
    "#                                                                               client_secret=client_secret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to pull the lyrics to my playlists, there are a few steps that need to be done.\n",
    "From Spotify:\n",
    "1) Pull the playlist info\n",
    "\n",
    "2) Pull the track names for each song\n",
    "\n",
    "3) Pull the artists names for each song\n",
    "\n",
    "\n",
    "THEN\n",
    "\n",
    "4) Using the Genius API, the song name and artist, pull the song information\n",
    "\n",
    "5) Check whether the tracks are included on the Genius API\n",
    "\n",
    "6) If they're included, pull the url for the song's page\n",
    "\n",
    "7) Scrape the lyrics from the page\n",
    "\n",
    "8) Finally, return the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class GetLyrics():\n",
    "    \n",
    "#     def __init__(self, spotify_client_id, spotify_client_secret, user_id, playlist_id, genius_key):\n",
    "#         self.spotify_client_id = spotify_client_id\n",
    "#         self.spotify_client_secret = spotify_client_secret\n",
    "#         self.user_id = user_id\n",
    "#         self.playlist_id = playlist_id\n",
    "#         self.genius_key = genius_key\n",
    "        \n",
    "#     def get_playlist_info(self):\n",
    "#         token = SpotifyClientCredentials(client_id=self.spotify_client_id, client_secret=self.spotify_client_secret).get_access_token()\n",
    "#         sp = spotipy.Spotify(token)\n",
    "#         playlist = sp.user_playlist_tracks(self.user_id, self.playlist_id)\n",
    "#         self.playlist = playlist\n",
    "#         return self.playlist\n",
    "    \n",
    "#     def get_track_names(self):\n",
    "#         track_names = []\n",
    "#         for song in range(len(self.playlist['items'])):\n",
    "#             track_names.append(self.playlist['items'][song]['track']['name'])\n",
    "#         self.track_names = track_names\n",
    "#         return self.track_names\n",
    "    \n",
    "#     def get_track_artists(self):\n",
    "#         track_artists = []\n",
    "#         for song in range(len(self.playlist['items'])):\n",
    "#             track_artists.append(self.playlist['items'][song]['track']['artists'][0]['name'])\n",
    "#         self.track_artists = track_artists\n",
    "#         return self.track_artists\n",
    "        \n",
    "#     def request_song_info(self, track_name, track_artist):\n",
    "#         self.track_name = track_name\n",
    "#         self.track_artist = track_artist\n",
    "#         base_url = 'https://api.genius.com'\n",
    "#         headers = {'Authorization': 'Bearer ' + self.genius_key}\n",
    "#         search_url = base_url + '/search'\n",
    "#         data = {'q': track_name + ' ' + track_artist}\n",
    "#         response = requests.get(search_url, data=data, headers=headers)\n",
    "#         self.response = response\n",
    "#         return self.response\n",
    "\n",
    "#     def check_hits(self):\n",
    "#         json = self.response.json()\n",
    "#         remote_song_info = None\n",
    "#         for hit in json['response']['hits']:\n",
    "#             if self.track_artist.lower() in hit['result']['primary_artist']['name'].lower():\n",
    "#                 remote_song_info = hit\n",
    "#                 break\n",
    "#         self.remote_song_info = remote_song_info\n",
    "#         return self.remote_song_info\n",
    "    \n",
    "#     def get_url(self):\n",
    "#         song_url = self.remote_song_info['result']['url']\n",
    "#         self.song_url = song_url\n",
    "#         return self.song_url\n",
    "    \n",
    "#     def scrape_lyrics(self):\n",
    "#         page = requests.get(self.song_url)\n",
    "#         html = BeautifulSoup(page.text, 'html.parser')\n",
    "#         lyrics1 = html.find(\"div\", class_=\"lyrics\")\n",
    "#         lyrics2 = html.find(\"div\", class_=\"Lyrics__Container-sc-1ynbvzw-2 jgQsqn\")\n",
    "#         if lyrics1:\n",
    "#             lyrics = lyrics1.get_text()\n",
    "#         elif lyrics2:\n",
    "#             lyrics = lyrics2.get_text()\n",
    "#         elif lyrics1 == lyrics2 == None:\n",
    "#             lyrics = None\n",
    "#         return lyrics\n",
    "\n",
    "#     def get_lyrics(self):\n",
    "#         playlist = GetLyrics.get_playlist_info(self)\n",
    "#         track_names = GetLyrics.get_track_names(self)\n",
    "#         track_artists = GetLyrics.get_track_artists(self)\n",
    "#         song_lyrics = []\n",
    "#         for i in range(len(self.track_names)):\n",
    "#             print(\"\\n\")\n",
    "#             print(f\"Working on track {i}.\")\n",
    "#             response = GetLyrics.request_song_info(self, self.track_names[i], self.track_artists[i])\n",
    "#             remote_song_info = GetLyrics.check_hits(self)\n",
    "#             if remote_song_info == None:\n",
    "#                 lyrics = None\n",
    "#                 print(f\"Track {i} is not in the Genius database.\")\n",
    "#             else:\n",
    "#                 url = GetLyrics.get_url(self)\n",
    "#                 lyrics = GetLyrics.scrape_lyrics(self)\n",
    "#                 if lyrics == None:\n",
    "#                     print(f\"Track {i} is not in the Genius database.\")\n",
    "#                 else:\n",
    "#                     print(f\"Retrieved track {i} lyrics!\")\n",
    "#             song_lyrics.append(lyrics)\n",
    "#         return song_lyrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I put this into action, pulling both playlists separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songs2019 = GetLyrics(spotify_client_id, spotify_client_secret, spotify_user_id, spotify_playlist_id, genius_key)\n",
    "# song_lyrics2019 = songs2019.get_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spotify_playlist_id = 'spotify:playlist:37i9dQZF1Ejy4HBpGZqQ6f'\n",
    "# songs2018 = GetLyrics(spotify_client_id, spotify_client_secret, spotify_user_id, spotify_playlist_id, genius_key)\n",
    "# song_lyrics2018 = songs2018.get_lyrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to double check it had worked, so I checked the first two tracks. Turns out those were Juice by Lizzo and Finesse by Bruno Mars. Unexpected, but who am I to argue with the Spotify algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# song_lyrics2019[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# song_lyrics2018[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the two results and checking the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# song_lyrics = song_lyrics2019 + song_lyrics2018\n",
    "# len(song_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I noticed that I would need to clean up the data a bit. There were a few labels to call out verses/choruses/featured artists, so I wanted to get ride of those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = song_lyrics[0]\n",
    "# re.findall('\\[.*\\]', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# section_labels = [re.findall('\\[[^\\]]+\\]', song) for song in song_lyrics if song is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought it might be wise to count the instances of these words and take a look at them before I dropped them in case I was getting rid of something that might later become important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_counter = {}\n",
    "\n",
    "# for song in section_labels:\n",
    "#     for label in song:\n",
    "#         if label not in label_counter:\n",
    "#             label_counter[label] = 1\n",
    "#         else:\n",
    "#             label_counter[label] += 1\n",
    "# label_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, I was comfortable getting rid of these words. Additionally, I eliminated the new lines, and then added spaces in their place. \n",
    "\n",
    "\n",
    "Later when I would start generating lyrics, I found that it used profanity quite liberally, so I thought it might be wise to clean up the language a bit to make it less colourful. I used an additional Python library to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without_labels = [re.sub('\\[[^\\]]+\\]', '', song) for song in song_lyrics if song is not None]\n",
    "# without_newline = [re.sub('\\n', ' ', song) for song in without_labels]\n",
    "# respace_words = [re.sub('([a-z])([A-Z])', \"\\g<1> \\g<2>\", song) for song in without_newline]\n",
    "# lyrics_final = [profanity.censor(song) for song in respace_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I added the lyrics to a dataframe to make them easier for the algorithm to process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mirror, mirror on the wall Don't say it ’cause...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've never been closer I tried to understand T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Steppin' out the weekends open wide Fill it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I hold you, I touch you In a maze, can't fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have you heard that there's an ad Listed in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lines\n",
       "0  Mirror, mirror on the wall Don't say it ’cause...\n",
       "1  I've never been closer I tried to understand T...\n",
       "2    Steppin' out the weekends open wide Fill it ...\n",
       "3     I hold you, I touch you In a maze, can't fi...\n",
       "4    Have you heard that there's an ad Listed in ..."
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics = pd.DataFrame(lyrics_final, columns=['lines'])\n",
    "lyrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the data is ready, so I saved it. In the end, I maanged to pull 137 / 200 songs (the remainder of which weren't in the Genius database). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137, 1)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lyrics.to_csv('../data/lyrics', index=False)\n",
    "lyrics = pd.read_csv('../data/lyrics')\n",
    "\n",
    "lyrics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Ive created the x train and y train set. The x train set goes up to the penultimate word in the line, so the target is the final word. \n",
    "The y train set starts from the second word in the line to compensate for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = [line[:-1] for line in lyrics.lines]\n",
    "# y_train = [line[1:] for line in lyrics.lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the model to process the data, it needs numbers instead of words, so I used the tokenizer to assign each word an index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lyrics.lines)\n",
    "# x_train = tokenizer.texts_to_sequences(x_train)\n",
    "# y_train = tokenizer.texts_to_sequences(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to switch between the index and words eventually, so the below will assign the \n",
    "index and retrieve word from the index later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = tokenizer.word_index\n",
    "index2word = {value: key for key, value in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index[\"<pad>\"] = 0\n",
    "index2word[0] = \"<pad>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I've defined some parameters for the model below using the describe function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = []\n",
    "\n",
    "# for sequence in x_train:\n",
    "#     lengths.append(len(sequence))\n",
    "    \n",
    "# lengths = pd.Series(lengths)\n",
    "# lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len = 769\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "# embedding_dim = 128\n",
    "# vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = pad_sequences(x_train, maxlen=max_len, padding='post', truncating='post')\n",
    "# y_train = pad_sequences(y_train, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the model is built. the Sequential function groups a linear stack of layers into a model. We add the GRU which stands for Gated Recurrant Unit, and ask this only to return the last word in the sequence of lyrics rather than the whole line.  The GRU refers to the type of recurrent neural network used here, and the GRU is used instead of a Long short term memory (LSTM) on account of the fact that its faster to learn, but a disadvantage is that they have a shorter term memory for sequences. The model is then optimized to increase speed.\n",
    "\n",
    "\n",
    "Then, we feed it the data. The first time I did this I used only half the data (before I decided to use 2018 data too), and 50 epochs. This took around half an hour. \n",
    "\n",
    "Later on, I retrained with twice the data and 75 epochs - this took 2-3 hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True))\n",
    "# model.add(GRU(units=1024, return_sequences=True))\n",
    "# model.add(Dense(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=Adam(), loss=SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(x_train, y_train, epochs=75, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to retrain the model, you can load the model below. h5 is the first one I trained, h6 is too big to upload to GitHub, so you will have to use h5 instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model.h6\")\n",
    "model = load_model(\"model.h5\")\n",
    "# model = load_model(\"model.h6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is going to be fed a word, and based on that word, the model will produce a list of indexes (of words) weighted by how likely they are to follow the initial word given the lyrics in the songs we fed it. \n",
    "\n",
    "\n",
    "In order that it doesn't loop back on itself, for example:\n",
    "\n",
    "'I like disco music and I like to dance'\n",
    "\n",
    "If this was fed to the model without tweaking, from the first part of the sentence you can see that 'disco' is most likely to follow 'like' which is why it was chosen. However, when 'I like' appears again, then the model would want to choose 'disco' again, because that is the most likely word to follow 'like' as we've already seen.\n",
    "\n",
    "In order to avoid this, the next function will find the top x most likely indices (words) to follow, and will choose a random index (word) from that list instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_next_word(predictions):\n",
    "    '''\n",
    "    input: list of numbers showing weighting or prediction to be next word, indices correspond to \n",
    "    words \n",
    "    output: random index from top 5 predicted words \n",
    "    '''\n",
    "    return random.choice(sorted(list(zip(predictions, \n",
    "                                         range(len(predictions)))), reverse = True)[:7])[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the fun part. The below function calls on the model to generate the new lyrics. Given any word (as long as its in the original corpus) a new sequence will be generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ablaze mine place alone at me the tricks at me magic central is please ever night they'd here wealth need at can yellow funky place night ever is please central it need at me 'round good fever what the ever is please feeling it what get place mine funky hot None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate(word):\n",
    "    inputs = np.zeros((1, 1))\n",
    "    inputs[0, 0] = word2index[word]\n",
    "    count = 1\n",
    "    while count <= 50:\n",
    "        pred = model.predict(inputs)\n",
    "        next_word_index = select_next_word(list(pred[0][0]))\n",
    "        \n",
    "        inputs[0, 0] = next_word_index\n",
    "        print(index2word[next_word_index], end=\" \")\n",
    "        count += 1\n",
    "\n",
    "print(generate('mirror'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as you can see, the lyrics are a little more Bohemian Rhapsody than Bob Dylan, but as a first attempt, I'm pleased with the results. \n",
    "\n",
    "To take this further, id be interested to increase the size of the original corpus anad see the impact that this has. \n",
    "\n",
    "Likewise, basing the predicted word on only the word before it means that it will be difficult to get a natural, coherent result, and so further investigation for me would be to see whether I can base the prediction on the 2 or 3 words prior. \n",
    "\n",
    "I've included some links here to articles which helped me a lot with this project:\n",
    "\n",
    "https://towardsdatascience.com/become-a-lyrical-genius-4362e7710e43\n",
    "https://towardsdatascience.com/generating-eminem-lyrics-using-neural-networks-96e7f9c45e8a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
